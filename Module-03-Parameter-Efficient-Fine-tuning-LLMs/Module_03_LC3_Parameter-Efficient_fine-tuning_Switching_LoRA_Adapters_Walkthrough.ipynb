{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3520c9d6-0367-49fe-bcf6-02d6f5ad13fb",
   "metadata": {},
   "source": [
    "# Load and Switch PEFT LoRA Adapters on SLMs\n",
    "\n",
    "Here we will use the two LoRA adapters we built previously and show you how you can efforlessly switch between different NLP tasks by keeping the same base SLM and just switching your LoRA task-based adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d84cc43-2d77-4e28-886a-eb41690c730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoModelForTokenClassification, \n",
    "                          AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer,\n",
    "                          pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d700bf0-a670-4ef3-8322-27545f84c046",
   "metadata": {},
   "source": [
    "## Load Classification LoRA Adapter on Base BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e53707d-6075-48e2-b5e4-7bd5af26e4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased',\n",
    "                                                                id2label=id2label,\n",
    "                                                                label2id=label2id,\n",
    "                                                                num_labels=2)\n",
    "cls_tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased', \n",
    "                                          fast=True)\n",
    "\n",
    "cls_model.load_adapter(peft_model_id='./Solutions/qlora-distilbert-sentiment-adapter', # folder where model is saved\n",
    "                       adapter_name='sentiment-classifier')\n",
    "cls_model = cls_model.eval()\n",
    "cls_model = cls_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c5e1c0-3dbf-4920-abda-a403a829c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "cls_pipe = pipeline(task='text-classification', \n",
    "               model=cls_model, tokenizer=cls_tokenizer, \n",
    "               device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5d024-9e26-4a14-bf9a-67992a1e36ce",
   "metadata": {},
   "source": [
    "## Load NER LoRA Adapter on Base BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb1bac8-74fd-4be5-8e7c-3ddf002c18b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}\n",
    "\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", \n",
    "    num_labels=13, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    ")\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased', \n",
    "                                          fast=True)\n",
    "\n",
    "ner_model.load_adapter(peft_model_id='./Solutions/qlora-distilbert-ner-adapter', # folder where model is saved\n",
    "                       adapter_name='ner')\n",
    "ner_model = ner_model.eval()\n",
    "ner_model = ner_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082f808f-676c-4758-a017-1560a713821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "ner_pipe = pipeline(task='ner', \n",
    "               model=ner_model, tokenizer=ner_tokenizer, \n",
    "               device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417702b0-1c58-474a-a6f1-6b796985f07b",
   "metadata": {},
   "source": [
    "## Test same model for different tasks based on the different loaded adapters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec022f16-d826-4c2e-b114-55d454967894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9446002840995789}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pipe('This is definitely not a good movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "532af868-c1c0-481f-8155-77d1a01fbb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-corporation',\n",
       "  'score': 0.55284154,\n",
       "  'index': 1,\n",
       "  'word': 'google',\n",
       "  'start': 0,\n",
       "  'end': 6},\n",
       " {'entity': 'B-person',\n",
       "  'score': 0.8707909,\n",
       "  'index': 6,\n",
       "  'word': 'larry',\n",
       "  'start': 30,\n",
       "  'end': 35},\n",
       " {'entity': 'I-person',\n",
       "  'score': 0.9641062,\n",
       "  'index': 7,\n",
       "  'word': 'page',\n",
       "  'start': 36,\n",
       "  'end': 40},\n",
       " {'entity': 'B-person',\n",
       "  'score': 0.566822,\n",
       "  'index': 9,\n",
       "  'word': 'sergey',\n",
       "  'start': 45,\n",
       "  'end': 51},\n",
       " {'entity': 'I-person',\n",
       "  'score': 0.93272555,\n",
       "  'index': 10,\n",
       "  'word': 'br',\n",
       "  'start': 52,\n",
       "  'end': 54},\n",
       " {'entity': 'I-person',\n",
       "  'score': 0.6995924,\n",
       "  'index': 11,\n",
       "  'word': '##in',\n",
       "  'start': 54,\n",
       "  'end': 56}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe('Google Search was invented by Larry Page and Sergey Brin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94132bd4-0887-4907-87de-5a94cd4e3d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-location',\n",
       "  'score': 0.9541195,\n",
       "  'index': 5,\n",
       "  'word': 'san',\n",
       "  'start': 14,\n",
       "  'end': 17},\n",
       " {'entity': 'I-location',\n",
       "  'score': 0.79591817,\n",
       "  'index': 6,\n",
       "  'word': 'francisco',\n",
       "  'start': 18,\n",
       "  'end': 27},\n",
       " {'entity': 'B-corporation',\n",
       "  'score': 0.6386267,\n",
       "  'index': 12,\n",
       "  'word': 'apple',\n",
       "  'start': 58,\n",
       "  'end': 63},\n",
       " {'entity': 'B-corporation',\n",
       "  'score': 0.6593133,\n",
       "  'index': 14,\n",
       "  'word': 'microsoft',\n",
       "  'start': 68,\n",
       "  'end': 77}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am going to San Francisco for a conference organized by Apple and Microsoft\"\n",
    "ner_pipe(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
