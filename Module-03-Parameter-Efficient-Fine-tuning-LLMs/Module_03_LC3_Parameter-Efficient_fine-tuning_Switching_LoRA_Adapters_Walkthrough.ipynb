{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3520c9d6-0367-49fe-bcf6-02d6f5ad13fb",
   "metadata": {},
   "source": [
    "# Load and Switch PEFT LoRA Adapters on SLMs\n",
    "\n",
    "Here we will use the two LoRA adapters we built previously and show you how you can efforlessly switch between different NLP tasks by keeping the same base SLM and just switching your LoRA task-based adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d84cc43-2d77-4e28-886a-eb41690c730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoModelForTokenClassification, \n",
    "                          AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer,\n",
    "                          pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d700bf0-a670-4ef3-8322-27545f84c046",
   "metadata": {},
   "source": [
    "## Load Classification LoRA Adapter on Base BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e53707d-6075-48e2-b5e4-7bd5af26e4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased',\n",
    "                                                                id2label=id2label,\n",
    "                                                                label2id=label2id,\n",
    "                                                                num_labels=2)\n",
    "cls_tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased', \n",
    "                                          fast=True)\n",
    "\n",
    "cls_model.load_adapter(peft_model_id='qlora-distilbert-sentiment-adapter',\n",
    "                       adapter_name='sentiment-classifier')\n",
    "cls_model = cls_model.eval()\n",
    "cls_model = cls_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c5e1c0-3dbf-4920-abda-a403a829c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pipe = pipeline(task='text-classification', \n",
    "               model=cls_model, tokenizer=cls_tokenizer, \n",
    "               device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5d024-9e26-4a14-bf9a-67992a1e36ce",
   "metadata": {},
   "source": [
    "## Load NER LoRA Adapter on Base BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb1bac8-74fd-4be5-8e7c-3ddf002c18b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}\n",
    "\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", \n",
    "    num_labels=13, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    ")\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased', \n",
    "                                          fast=True)\n",
    "\n",
    "ner_model.load_adapter(peft_model_id='qlora-distilbert-ner-adapter',\n",
    "                       adapter_name='ner')\n",
    "ner_model = ner_model.eval()\n",
    "ner_model = ner_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082f808f-676c-4758-a017-1560a713821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipe = pipeline(task='ner', \n",
    "               model=ner_model, tokenizer=ner_tokenizer, \n",
    "               device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417702b0-1c58-474a-a6f1-6b796985f07b",
   "metadata": {},
   "source": [
    "## Test same model for different tasks based on the different loaded adapters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec022f16-d826-4c2e-b114-55d454967894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9594546556472778}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pipe('This is definitely not a good movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "532af868-c1c0-481f-8155-77d1a01fbb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-corporation',\n",
       "  'score': 0.7544448,\n",
       "  'index': 1,\n",
       "  'word': 'google',\n",
       "  'start': 0,\n",
       "  'end': 6},\n",
       " {'entity': 'B-person',\n",
       "  'score': 0.92128485,\n",
       "  'index': 6,\n",
       "  'word': 'larry',\n",
       "  'start': 30,\n",
       "  'end': 35},\n",
       " {'entity': 'I-person',\n",
       "  'score': 0.9446377,\n",
       "  'index': 7,\n",
       "  'word': 'page',\n",
       "  'start': 36,\n",
       "  'end': 40},\n",
       " {'entity': 'B-person',\n",
       "  'score': 0.9292107,\n",
       "  'index': 9,\n",
       "  'word': 'sergey',\n",
       "  'start': 45,\n",
       "  'end': 51},\n",
       " {'entity': 'I-person',\n",
       "  'score': 0.90647864,\n",
       "  'index': 10,\n",
       "  'word': 'br',\n",
       "  'start': 52,\n",
       "  'end': 54},\n",
       " {'entity': 'I-person',\n",
       "  'score': 0.49488345,\n",
       "  'index': 11,\n",
       "  'word': '##in',\n",
       "  'start': 54,\n",
       "  'end': 56}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe('Google Search was invented by Larry Page and Sergey Brin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
